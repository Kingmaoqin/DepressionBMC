Reviewer 1Thank you for the opportunity to review this manuscript. From my understanding, the work aims to advance AI explainability for depression treatment choice by applying counterfactual explanations (CFs) to examine how baseline HAM-D-17 items relate to SSRI vs SNRI selection. The authors benchmark 17 classifiers, select a Random Forest eventually, and use DICE to generate CFs for both “local” (patient-level) and “global” (population-level) interpretability. The idea - making model behavior tangible for clinicians via CFs - is timely and relevant to clinical decision support in MDD. To ensure the results are accurately reported and the conclusions match the data, several clarifications and corrections would substantially strengthen the manuscript. Major issuesFraming of the main outcome (RCT assignments are not the same clinician prescriptions): from my reading, the dataset pools 10 randomized clinical trials, yet the narrative repeatedly describes the label as “prescribed by clinicians” or “medication selection”, including in the Conclusion section. Because trial participation determines arm/class assignment, the observed label primarily reflects randomization rather than a “free” clinician prescription. I suggest reframing throughout the manuscript to make clear that the model predicts the observed class in a pooled RCT dataset, and that CFs analyze model-based “what-if” scenarios. If the authors intend to treat arm assignment as a proxy for clinical selection, a brief justification explaining why this approximation is reasonable would help align claims with the data source.Response:We appreciate the reviewer's observation regarding the distinction between RCT assignments and clinical prescriptions. We acknowledge that medications in our dataset were assigned through randomization protocols. However, we note that the RCT protocols themselves embed meaningful clinical reasoning. The 10 pooled trials were designed with specific inclusion/exclusion criteria, dose titration schemes, and subgroup stratifications based on baseline symptom profiles including HAM-D severity, anxiety symptoms, sleep disturbances, and melancholic features. These protocol design choices reflect collective clinical expertise about which patient presentations might benefit from different medication classes and dosing strategies. For example, the dose escalation protocols for duloxetine 60mg versus 40mg groups were not arbitrary but designed considering symptom severity and tolerability profiles. By aggregating data across 10 trials with varied protocols, our model captures patterns of which symptom profiles historically appeared in which treatment arms, patterns that may reflect the clinical judgment embedded in these trial designs. We have revised the manuscript to clarify that our model predicts treatment assignment within RCT protocols rather than free clinical prescriptions, and we emphasize that counterfactual explanations represent hypothesis-generating findings requiring prospective validation in real-world clinical settings. We believe this reframing accurately represents our data source while maintaining the clinical relevance of identifying symptom-medication associations that warrant further investigation in observational studies where clinicians make unconstrained treatment decisions.Drug-class labeling for paroxetine at higher doses: page 8 and Table 2 introduce dose-dependent schemes that classify paroxetine ≥50 mg (V1) or >50 mg (V2) as SNRI. This is unconventional and unreferenced, and it could materially affect class balance and conclusions. The authors could either provide peer-reviewed pharmacologic support for this choice or revert to standard molecule-based classes as the primary analysis, presenting the dose-based schemes as sensitivity analyses. Reporting all three schemas (name-based, Dosing V1, Dosing V2) would let readers judge robustness.Response:We appreciate the reviewer's request for pharmacologic justification of our dose-dependent classification scheme. There is substantial peer-reviewed evidence that paroxetine exhibits dose-dependent norepinephrine transporter inhibition in addition to serotonin reuptake inhibition, with clinically meaningful norepinephrine activity emerging at higher doses. However, we note that in our dataset, paroxetine was administered exclusively at 20mg daily across all trials, meaning our dose-based classification thresholds of 50mg did not affect any paroxetine-treated patients and therefore did not impact class balance. We have revised the manuscript to present all three classification schemas with more detailed descriptions.DrugDosage (mg/day)Count (N)Schema A(Name-based)Schema B(Dosing V1)Schema C(Dosing V2)Duloxetine60583SNRISNRISSRIDuloxetine80170SNRISNRISNRIDuloxetine120177SNRISNRISNRIVenlafaxine75140SNRISSRISSRIFluoxetine2045SSRISSRISSRIParoxetine20353SSRISSRISSRIReferences:Owens, Michael J., et al. "Estimates of serotonin and norepinephrine transporter inhibition in depressed patients treated with paroxetine or venlafaxine." Neuropsychopharmacology 33.13 (2008): 3201-3212.Gilmor, Michelle L., Michael J. Owens, and Charles B. Nemeroff. "Inhibition of norepinephrine uptake in patients with major depression treated with paroxetine." American Journal of Psychiatry 159.10 (2002): 1702-1710.Evaluation hygiene and contradictions (SMOTE scope, confusion matrix, overfitting): Table 3’s caption notes one-hot encoding, oversampling, and 5-fold CV, but the timing and scope of oversampling are not explicit. If SMOTE touched validation/test folds, performance would probably be inflated. The current confusion matrix in Fig. 4 appears to be built on an oversampled distribution (balanced counts of 1121/1121 and 974 correct SNRIs) that conflicts with the true Dosing V2 class totals (347 SNRIs / 1121 SSRIs), which is confusing for readers.Table 3 indicates that the training metrics are almost at 1.0, alongside test AUC ~0.93 and other test metrics ~0.85; this suggests either leakage or substantial overfitting (or that the reported “training results” reflect performance on the entire training set after fitting, rather than the cross-validation training folds, which would overstate accuracy). Considering all of the above, it would strengthen the paper to: (a) state explicitly that oversampling occurs inside training folds only (never on validation/test), and, if needed, re-run and re-report; (b) recompute confusion matrices on the original, non-oversampled test folds so counts match the true class frequencies; and (c) report mean ± SD across folds, class-wise precision/recall, balanced accuracy, and calibration (e.g., ECE/Brier or a reliability curve) to document generalization faithfully.Response:We appreciate the reviewer’s opinions regarding our evaluation, and we have fully addressed these concerns by re-executing our entire experimental pipeline. We have explicitly restructured our cross-validation workflow so that SMOTE oversampling is applied exclusively within the training folds, ensuring that all validation and test folds remain entirely unseen and non-oversampled to prevent any data leakage. Furthermore, we have recomputed the confusion matrices using the original, raw test data to accurately reflect the true clinical class imbalance rather than the synthetic training distribution. We have expanded our results to include mean ± standard deviation across folds, class-wise precision/recall, balanced accuracy, and calibration metrics (Brier score) for both training and test sets. These updated results have been fully incorporated into the revised manuscript and figures.Cross-trial confounding and split strategy: because multiple RCTs are pooled, a random split can let the model learn trial identity (e.g., site patterns, eligibility windows, dosing ranges) rather than drug-class signal; the shifting class totals across labeling schemes make this especially plausible. If trial-specific structure leaks into both train and test, reported performance may be optimistic and CFs may reflect trial artifacts rather than class-related symptom structure. Evaluating with grouped cross-validation by trial ID (e.g., GroupKFold) and reporting those results alongside the usual CV would directly address this concern and clarify how much signal generalizes across trials.Response:We appreciate the reviewer’s insightful comment. To directly address this concern, we have conducted an analysis using GroupKFold cross-validation, strictly grouping data by Trial ID to ensure that the model is tested on entirely unseen trials.We have updated the manuscript with these new experiments, including a comparative visualization illustrating the performance distribution under both standard Stratified K-Fold and trial-based GroupKFold strategies. The results indicate that the performance metrics remained highly consistent between the random split and the trial-based split. This consistency confirms that models have successfully learned generalizable symptom-drug associations rather than trial identity. Counterfactuals - method and language: The methods section says CFs are generated “using gradient descent”, but Random Forests are non-differentiable; readers would gain from the actual DICE procedure (e.g., black-box/heuristic search) and its settings: k (CFs per instance), λ₁/λ₂ (loss weights), restarts, step limits, feasibility constraints, random seeds, and failure criteria (either in the main text or within an Appendix). The diversity term also needs a correction in intuition: with a kernel like Kij = 1/(1 + dist(Xi′, Xj′)), more-similar CFs typically make K more rank-deficient and decrease det K; diversity (larger distances) increases det K. A simple sentence, such as “farther-apart CFs increase the determinant, encouraging diversity”- would resolve this.Finally, defining the “target class” better and earlier would aid in the paper’s readability (e.g., a one-liner before Eq. 1 and once more before Fig. 5: “the target class is the opposite antidepressant class relative to the model’s current prediction”). Throughout the Abstract/Discussion/Conclusion, temper causal phrasing (e.g., “causally influence,” “simulate causal relationships”): CFs here are model-based what-ifs, not identified causal effects. Replacing with “associated in the model”, “used by the model to distinguish classes”, or “model-implied adjustments” and noting in Limitations that causal inference was not performed, would align language with methods better.Response:We appreciate the reviewer's precise technical correction and agree that the reference to "gradient descent" was inaccurate given our use of non-differentiable Random Forest models. We have clarified in the Methods section that we employed the standard DICE framework utilizing the method="random" strategy, which acts as a model-agnostic search. Instead of relying on gradients, this approach randomly samples the feature space within the constraints of the training data distribution to identify valid counterfactuals that successfully flip the prediction class while minimizing the distance from the original query instance.Furthermore, we have accepted your suggestions regarding the manuscript's terminology and definitions. We have corrected the mathematical intuition regarding the diversity term to clarify that farther-apart counterfactuals increase the determinant, explicitly defined the "target class" as the opposite prediction, and systematically tempered all causal phrasing throughout the manuscript.Representation and distances for ordinal HAM-D items: Table 1 presents ordinal scoring (0-2/3/4), but the manuscript alternates between continuous, categorical, and one-hot treatments of these data. Inconsistent representation between model training and the CF distance can yield implausible suggestions (e.g., large multi-step jumps treated as equivalent to single-step changes). Using a consistent encoding for both training and CF distance, and a cost that respects ordinality (e.g., 0->1 “cheaper” than 0->4), would improve plausibility. When reporting CF deltas, including each item’s scoring range (e.g., “increase HAM-D12 by 2 [0-2 scale]”) would further aid clinical interpretation.Response:We appreciate the reviewer’s valuable comments. In our revised implementation, we have standardized the representation of all HAM-D items as numerical variables rather than using categorical one-hot encoding. This approach preserves ordinality because the distance metric used for generating counterfactuals naturally calculates a proportionately higher cost for larger multi-step jumps (e.g., a change from 0 to 4) compared to single-step changes (e.g., 0 to 1), thereby discouraging implausible drastic shifts and favoring minimal necessary adjustments. Furthermore, to facilitate clinical interpretation as suggested, we have updated our reporting and visualizations to display the direction and magnitude of score changes alongside the valid scoring range for each symptom item.Results and presentation inconsistencies: the Abstract summarizes “accuracy, F1, precision, recall, ROC-AUC near 0.85”, whereas Table 3 reports ROC-AUC ~0.93 for the Random Forest classifier; reconciling this (for example, “most test metrics ~0.85, with ROC-AUC ~0.93”) would avoid confusion. In Fig. 6(b), HAM-D16 is labeled “lack of insight,” while Table 1 defines HAM-D16 = weight loss and HAM-D17 = lack of insight; the body text and the Conclusion section also refer to HAM-D16 as weight loss. Standardizing item labels across figures, text, and Conclusion would tighten the presentation. The statement that the dataset is “devoid of anomalies or missing values” would benefit from a brief note on checks/imputation policy, or softer wording if appropriate. Response:We are grateful for the reviewer's comments and have fully resolved these presentation inconsistencies. We have revised the Abstract to accurately reflect the performance nuances, ensuring strict alignment with the specific results reported in Table 3. Furthermore, we have standardized the variable labeling across all figures, tables, and the Conclusion to correctly consistently identify HAM-D16 and HAM-D17. Lastly, we have clarified in the Clinical Trial Depression Dataset section that the dataset contains no missing values for the features of interest (HAM-D items, drug class, and dosage) following a rigorous pre-processing step.“Expert-centered evaluation”: the manuscript mentions an expert evaluation for realism/actionability, but provides no Methods/Results (e.g., sample size, criteria, inter-rater agreement, etc.). A short, auditable paragraph describing the procedure and findings (or removing the claim) would resolve this gap.Response:We appreciate the reviewer’s guidance on substantiating our expert evaluation claims and have addressed this gap by adding the necessary methodological details. We have updated the manuscript, specifically within the sections discussing feature importance, to include a clear description of our validation procedure. As now clarified in the manuscript, multiple medical experts from our research team conducted a review to evaluate the alignment between the model-derived feature importance and established medical knowledge, thereby confirming the clinical plausibility of the identified predictors.Clinical tone and figures: some diagrams/text (especially the early schematic) currently read like treatment recommendations (“You should take SSRI/SNRI”). Rephrasing this as model predictions or counterfactual scenarios, and avoiding “proactive” (“real-life”) prescriptive language, would prevent misinterpretation as clinical advice.Response:We thank the reviewer for this critical observation regarding the clinical tone and potential for misinterpretation. We have thoroughly revised the entire manuscript to strictly avoid any prescriptive language that could be construed as medical advice.Minor issues (“nice-to-have”s)Uncertainty and model comparisons: adding bootstrap 95% CIs for primary metrics, and noting that 17 models were compared without multiplicity control, would help readers avoid over-interpreting small differences - especially when confidence intervals overlap.Stability of global CF rankings: indicating whether the global CF frequency ranking (top-k symptoms) is stable across random seeds (and ideally across the three labeling schemes) would reassure readers that these summaries are not artifacts of initialization or label definition.Figure polish: in schematic figures, consider an explicit on-figure label such as “Model prediction: SSRI/SNRI,” and, when displaying CF deltas, show each item’s score range for quick clinical interpretation, as explained above.Response:We appreciate these constructive suggestions. We have updated the manuscript to include bootstrap 95% confidence intervals for all primary performance metrics. Furthermore, we added specific stability experiments verifying that the global counterfactual feature rankings remain consistent across 10 random seeds and the three different labeling schemas. Finally, we have polished the schematic figures by adding explicit labels and clarifying score ranges to facilitate immediate clinical interpretation.Reviewer 2 This paper presents an interesting study that applies explainable counterfactual reasoning (CFs) to the selection of antidepressant medications for Major Depressive Disorder (MDD). The work effectively integrates concepts from explainable artificial intelligence (XAI) with clinical decision support, addressing both personalized and population-level analyses. However, in my view, several aspects require further clarification and improvement. 1. While the paper employs the DICE algorithm for antidepressant selection, the mathematical formulation and algorithmic steps appear identical to those in the original DICE framework. Consequently, it is difficult to identify what constitutes a novel methodological contribution as opposed to a straightforward application of existing techniques. To strengthen the paper as a methodologically oriented contribution, the authors should clearly specify any modifications or innovations introduced in their adaptation. Alternatively, if the primary focus is on the empirical findings derived from applying DICE to a specific dataset, the discussion should be expanded to provide deeper insights into the results and their clinical implications.Response:We appreciate the reviewer’s insightful comment regarding the positioning of our methodological contribution. While we utilize the foundational DICE method, our distinct contribution lies in the domain-specific adaptation of this technique for psychiatric clinical decision support. We also fully accept your valuable suggestion to strengthen the empirical narrative. Accordingly, we have significantly expanded the Discussion section to provide deeper clinical insights into the identified symptom-drug associations.2. The paper focuses exclusively on the DICE algorithm for generating counterfactual explanations, but it does not include any comparative analysis with other CF-based or explainability approaches. This limits the reader’s ability to assess the advantages or trade-offs of the proposed framework. Without such comparison, it remains unclear whether DICE truly provides better diversity, plausibility, or clinical interpretability for this depression dataset.Response:We thank the reviewer for this valuable suggestion to broaden the comparative scope of our interpretability framework. While our primary focus remains on counterfactual explanations via DICE due to their unique ability to provide actionable "what-if" scenarios for clinical decision support, we agree that contextualizing these results against established attribution-based methods is essential. In response, we have expanded our experimental pipeline to include a comprehensive comparative analysis with SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), utilizing SHAP to validate the global consistency of the top predictors and LIME to visualize local feature weights for specific patient instances. Those experiments demonstrate that while SHAP and LIME effectively highlight which symptoms drive a prediction, DICE uniquely adds value by quantifying how much a symptom must change to alter that clinical decision, and we have accordingly added a new subsection in the Results with visualization comparisons to demonstrate how these complementary XAI techniques reinforce the plausibility and clinical interpretability of our proposed framework.3. Random Forest is selected as the primary classifier since it outperformed other models in Table 3. However, the performance differences are small when compared with CatBoost, Stacking, Extra Trees, HistGradient Boosting, Voting, and Gaussian Process models. It would be valuable to demonstrate whether DICE yields consistent or differing counterfactual explanations when applied to these alternative models.Response:We appreciate the reviewer’s valuable comments. We have expanded our analysis to include four additiona models (CatBoost, Gradient Boosting, Extra Trees, and Stacking). While their performance metrics were comparable to Random Forest, we observed that the generated counterfactual explanations diverged across architectures. We interpret this phenomenon as the "Predictive Multiplicity," where multiple models achieve similar accuracy by exploiting different correlations within the high-dimensional symptom space. This finding underscores the necessity of our framework: since different "black boxes" arrive at decisions via different logical paths, clinicians must be presented with explanations specific to the deployed model. Consequently, we retained Random Forest as our primary model not only for its performance but because its decision logic (as revealed by DICE) was validated by our clinical experts as the most medically plausible. We have added a "Model Multiplicity and Stability" subsection to the Discussion to demonstrate these findings. 4. On page 8, the text states "larger det(K)", which should be corrected to "smaller det(K)".Response:We appreciate the reviewer's valuable comment. We have updated the manuscript on page 8 as suggested.
